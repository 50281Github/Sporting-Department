#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç°åœºéƒ¨-åˆ†æ‹£è¿›åº¦æ•°æ®é‡‡é›†å™¨
å®šæ—¶ä»è§‚éº¦ä¸šåŠ¡å¹³å°APIè·å–åˆ†æ‹£æ•°æ®å¹¶ä¿å­˜
"""

import requests
import json
import csv
import os
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any
import schedule

class DataCollector:
    def __init__(self, config_file='config.json'):
        """åˆå§‹åŒ–æ•°æ®é‡‡é›†å™¨"""
        self.config = self.load_config(config_file)
        self.setup_logging()
        self.session = requests.Session()
        self.setup_session()
        
    def load_config(self, config_file: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "api": {
                "base_url": "https://station.guanmai.cn",
                "endpoint": "/weight/weight_collect/weight_info/get",
                "time_config_id": "ST22071",
                "headers": {
                    "Accept": "application/json, text/plain, */*",
                    "Accept-Encoding": "gzip, deflate, br, zstd",
                    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Host": "station.guanmai.cn",
                    "Pragma": "no-cache",
                    "Referer": "https://station.guanmai.cn/",
                    "Sec-Ch-Ua": '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
                    "Sec-Ch-Ua-Mobile": "?0",
                    "Sec-Ch-Ua-Platform": '"Windows"',
                    "Sec-Fetch-Dest": "empty",
                    "Sec-Fetch-Mode": "cors",
                    "Sec-Fetch-Site": "same-origin",
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
                    "X-Guanmai-Request-Id": "1735462800000-1735462800000"
                }
            },
            "collection": {
                "interval_minutes": 5,
                "data_dir": "collected_data",
                "csv_filename": "sorting_progress_{date}.csv",
                "json_filename": "sorting_progress_{date}.json",
                "log_filename": "collector.log"
            },
            "retry": {
                "max_attempts": 3,
                "delay_seconds": 5
            }
        }
        
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    # åˆå¹¶é…ç½®
                    default_config.update(user_config)
            except Exception as e:
                print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return default_config
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        log_dir = self.config['collection']['data_dir']
        os.makedirs(log_dir, exist_ok=True)
        
        log_file = os.path.join(log_dir, self.config['collection']['log_filename'])
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_session(self):
        """è®¾ç½®è¯·æ±‚ä¼šè¯"""
        self.session.headers.update(self.config['api']['headers'])
    
    def get_target_date(self, offset_days: int = 0) -> str:
        """è·å–ç›®æ ‡æ—¥æœŸå­—ç¬¦ä¸²
        
        é€»è¾‘ï¼šæ¯å¤©18:00åè‡ªåŠ¨åˆ‡æ¢åˆ°æ¬¡æ—¥æ•°æ®
        - 18:00ä¹‹å‰ï¼šè¯·æ±‚å½“å¤©æ•°æ®
        - 18:00ä¹‹åï¼šè¯·æ±‚æ¬¡æ—¥æ•°æ®
        """
        now = datetime.now()
        
        # åˆ¤æ–­æ˜¯å¦å·²è¿‡18ç‚¹
        if now.hour >= 18:
            # 18ç‚¹åï¼Œè¯·æ±‚æ¬¡æ—¥æ•°æ®
            target_date = now + timedelta(days=1 + offset_days)
            self.logger.info(f"å½“å‰æ—¶é—´ {now.strftime('%H:%M')} å·²è¿‡18ç‚¹ï¼Œè¯·æ±‚æ¬¡æ—¥æ•°æ®: {target_date.strftime('%Y-%m-%d')}")
        else:
            # 18ç‚¹å‰ï¼Œè¯·æ±‚å½“å¤©æ•°æ®
            target_date = now + timedelta(days=offset_days)
            self.logger.info(f"å½“å‰æ—¶é—´ {now.strftime('%H:%M')} æœªè¿‡18ç‚¹ï¼Œè¯·æ±‚å½“å¤©æ•°æ®: {target_date.strftime('%Y-%m-%d')}")
        
        return target_date.strftime('%Y-%m-%d 00:00:00')
    
    def fetch_data(self, target_date: str = None) -> Dict[str, Any]:
        """ä»APIè·å–æ•°æ®"""
        if target_date is None:
            target_date = self.get_target_date()

        url = f"{self.config['api']['base_url']}{self.config['api']['endpoint']}"
        params = {
            'time_config_id': self.config['api']['time_config_id'],
            'target_date': target_date
        }

        for attempt in range(self.config['retry']['max_attempts']):
            try:
                self.logger.info(f"æ­£åœ¨è·å–æ•°æ® (å°è¯• {attempt + 1}/{self.config['retry']['max_attempts']})")
                self.logger.info(f"è¯·æ±‚URL: {url}")
                self.logger.info(f"è¯·æ±‚å‚æ•°: {params}")

                response = self.session.get(url, params=params, timeout=30)
                response.raise_for_status()

                data = response.json()
                self.logger.info(f"æ•°æ®è·å–æˆåŠŸï¼Œå“åº”å¤§å°: {len(response.text)} å­—ç¬¦")
                return {
                    'timestamp': datetime.now().isoformat(),
                    'target_date': target_date,
                    'data': data,
                    'status': 'success'
                }

            except requests.exceptions.RequestException as e:
                self.logger.error(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt < self.config['retry']['max_attempts'] - 1:
                    time.sleep(self.config['retry']['delay_seconds'])
                else:
                    return {
                        'timestamp': datetime.now().isoformat(),
                        'target_date': target_date,
                        'error': str(e),
                        'status': 'failed'
                    }
            except json.JSONDecodeError as e:
                self.logger.error(f"JSONè§£æå¤±è´¥: {e}")
                return {
                    'timestamp': datetime.now().isoformat(),
                    'target_date': target_date,
                    'error': f"JSONè§£æå¤±è´¥: {e}",
                    'status': 'failed'
                }
    
    def fetch_sorter_rank_data(self, cycle_start_time: str = None, cycle_end_time: str = None) -> Dict[str, Any]:
        """è·å–åˆ†æ‹£å‘˜æ’åæ•°æ®
        
        é€»è¾‘ï¼šä¸åˆ†æ‹£è¿›åº¦æ•°æ®ä¿æŒä¸€è‡´ï¼Œ18:00åè‡ªåŠ¨åˆ‡æ¢åˆ°æ¬¡æ—¥æ•°æ®
        """
        if cycle_start_time is None or cycle_end_time is None:
            now = datetime.now()
            
            # åˆ¤æ–­æ˜¯å¦å·²è¿‡18ç‚¹ï¼Œå†³å®šæŸ¥è¯¢å“ªä¸€å¤©çš„æ•°æ®
            if now.hour >= 18:
                # 18ç‚¹åï¼ŒæŸ¥è¯¢æ¬¡æ—¥çš„5:00-9:00æ•°æ®
                target_date = (now + timedelta(days=1)).strftime('%Y-%m-%d')
                self.logger.info(f"å½“å‰æ—¶é—´ {now.strftime('%H:%M')} å·²è¿‡18ç‚¹ï¼ŒæŸ¥è¯¢æ¬¡æ—¥åˆ†æ‹£å‘˜æ’åæ•°æ®: {target_date}")
            else:
                # 18ç‚¹å‰ï¼ŒæŸ¥è¯¢å½“å¤©çš„5:00-9:00æ•°æ®
                target_date = now.strftime('%Y-%m-%d')
                self.logger.info(f"å½“å‰æ—¶é—´ {now.strftime('%H:%M')} æœªè¿‡18ç‚¹ï¼ŒæŸ¥è¯¢å½“å¤©åˆ†æ‹£å‘˜æ’åæ•°æ®: {target_date}")
            
            cycle_start_time = f"{target_date} 05:00"
            cycle_end_time = f"{target_date} 09:00"

        url = f"{self.config['api']['base_url']}/weight/weight_collect/sorter/rank"
        params = {
            'time_config_id': self.config['api']['time_config_id'],
            'cycle_start_time': cycle_start_time,
            'cycle_end_time': cycle_end_time
        }

        for attempt in range(self.config['retry']['max_attempts']):
            try:
                self.logger.info(f"æ­£åœ¨è·å–åˆ†æ‹£å‘˜æ’åæ•°æ® (å°è¯• {attempt + 1}/{self.config['retry']['max_attempts']})")
                self.logger.info(f"è¯·æ±‚URL: {url}")
                self.logger.info(f"è¯·æ±‚å‚æ•°: {params}")

                response = self.session.get(url, params=params, timeout=30)
                response.raise_for_status()

                data = response.json()
                self.logger.info(f"åˆ†æ‹£å‘˜æ’åæ•°æ®è·å–æˆåŠŸï¼Œå“åº”å¤§å°: {len(response.text)} å­—ç¬¦")
                
                # ç»Ÿè®¡æ’åæ•°æ®
                if data.get('code') == 0 and isinstance(data.get('data'), list):
                    sorter_count = len(data['data'])
                    total_results = sum(item.get('statistic_results', 0) for item in data['data'])
                    self.logger.info(f"è·å–åˆ° {sorter_count} ååˆ†æ‹£å‘˜æ’åæ•°æ®ï¼Œæ€»è®¡å®Œæˆ {total_results} ä»¶")
                
                return {
                    'timestamp': datetime.now().isoformat(),
                    'cycle_start_time': cycle_start_time,
                    'cycle_end_time': cycle_end_time,
                    'data': data,
                    'status': 'success'
                }

            except requests.exceptions.RequestException as e:
                self.logger.error(f"åˆ†æ‹£å‘˜æ’åæ•°æ®è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt < self.config['retry']['max_attempts'] - 1:
                    time.sleep(self.config['retry']['delay_seconds'])
                else:
                    return {
                        'timestamp': datetime.now().isoformat(),
                        'cycle_start_time': cycle_start_time,
                        'cycle_end_time': cycle_end_time,
                        'error': str(e),
                        'status': 'failed'
                    }
            except json.JSONDecodeError as e:
                self.logger.error(f"åˆ†æ‹£å‘˜æ’åæ•°æ®JSONè§£æå¤±è´¥: {e}")
                return {
                    'timestamp': datetime.now().isoformat(),
                    'cycle_start_time': cycle_start_time,
                    'cycle_end_time': cycle_end_time,
                    'error': f"JSONè§£æå¤±è´¥: {e}",
                    'status': 'failed'
                }
    
    def save_sorter_rank_to_json(self, data: Dict[str, Any], date_str: str = None):
        """ä¿å­˜åˆ†æ‹£å‘˜æ’åæ•°æ®åˆ°JSONæ–‡ä»¶"""
        data_dir = self.config['collection']['data_dir']
        os.makedirs(data_dir, exist_ok=True)
        
        filename = "sorter_rank.json"
        filepath = os.path.join(data_dir, filename)
        
        # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¿½åŠ åˆ°æ•°ç»„ä¸­
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                if not isinstance(existing_data, list):
                    existing_data = [existing_data]
                existing_data.append(data)
            except:
                existing_data = [data]
        else:
            existing_data = [data]
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"åˆ†æ‹£å‘˜æ’åæ•°æ®å·²ä¿å­˜åˆ°JSONæ–‡ä»¶: {filepath}")
    
    def save_sorter_rank_to_csv(self, data: Dict[str, Any], date_str: str = None):
        """ä¿å­˜åˆ†æ‹£å‘˜æ’åæ•°æ®åˆ°CSVæ–‡ä»¶"""
        data_dir = self.config['collection']['data_dir']
        os.makedirs(data_dir, exist_ok=True)
        
        # ä¿å­˜åˆ†æ‹£å‘˜æ’åè¯¦ç»†æ•°æ®
        detail_filename = "sorter_rank_detail.csv"
        detail_filepath = os.path.join(data_dir, detail_filename)
        
        # ä¿å­˜åˆ†æ‹£å‘˜æ’åæ±‡æ€»æ•°æ®
        summary_filename = "sorter_rank_summary.csv"
        summary_filepath = os.path.join(data_dir, summary_filename)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå†³å®šæ˜¯å¦å†™å…¥è¡¨å¤´
        detail_file_exists = os.path.exists(detail_filepath)
        summary_file_exists = os.path.exists(summary_filepath)
        
        try:
            # ä¿å­˜è¯¦ç»†æ•°æ®è®°å½•
            with open(detail_filepath, 'a', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                
                # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå†™å…¥è¡¨å¤´
                if not detail_file_exists:
                    headers = [
                        'é‡‡é›†æ—¶é—´', 'å‘¨æœŸå¼€å§‹æ—¶é—´', 'å‘¨æœŸç»“æŸæ—¶é—´', 'APIçŠ¶æ€ç ', 'APIæ¶ˆæ¯',
                        'åˆ†æ‹£å‘˜å§“å', 'æ’å', 'å®Œæˆä»¶æ•°', 'å“åº”çŠ¶æ€', 'å¤‡æ³¨'
                    ]
                    writer.writerow(headers)
                
                # å†™å…¥è¯¦ç»†æ•°æ®è¡Œ
                api_data = data.get('data', {})
                if api_data.get('code') == 0 and isinstance(api_data.get('data'), list):
                    for sorter in api_data['data']:
                        row = [
                            data['timestamp'],
                            data['cycle_start_time'],
                            data['cycle_end_time'],
                            api_data.get('code', ''),
                            api_data.get('msg', ''),
                            sorter.get('sorter_name', ''),
                            sorter.get('rank', ''),
                            sorter.get('statistic_results', ''),
                            data['status'],
                            ''
                        ]
                        writer.writerow(row)
                else:
                    # å¦‚æœAPIè¿”å›é”™è¯¯æˆ–æ— æ•°æ®ï¼Œè®°å½•é”™è¯¯ä¿¡æ¯
                    row = [
                        data['timestamp'],
                        data['cycle_start_time'],
                        data['cycle_end_time'],
                        api_data.get('code', ''),
                        api_data.get('msg', ''),
                        '',
                        '',
                        '',
                        data['status'],
                        data.get('error', '')
                    ]
                    writer.writerow(row)
            
            # ä¿å­˜æ±‡æ€»ç»Ÿè®¡æ•°æ®
            with open(summary_filepath, 'a', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                
                # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå†™å…¥è¡¨å¤´
                if not summary_file_exists:
                    headers = [
                        'é‡‡é›†æ—¶é—´', 'å‘¨æœŸå¼€å§‹æ—¶é—´', 'å‘¨æœŸç»“æŸæ—¶é—´', 
                        'åˆ†æ‹£å‘˜æ€»æ•°', 'æ€»å®Œæˆä»¶æ•°', 'å¹³å‡å®Œæˆä»¶æ•°', 'å“åº”çŠ¶æ€'
                    ]
                    writer.writerow(headers)
                
                # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
                api_data = data.get('data', {})
                if api_data.get('code') == 0 and isinstance(api_data.get('data'), list):
                    sorter_count = len(api_data['data'])
                    total_results = sum(item.get('statistic_results', 0) for item in api_data['data'])
                    avg_results = total_results / sorter_count if sorter_count > 0 else 0
                else:
                    sorter_count = 0
                    total_results = 0
                    avg_results = 0
                
                row = [
                    data['timestamp'],
                    data['cycle_start_time'],
                    data['cycle_end_time'],
                    sorter_count,
                    total_results,
                    round(avg_results, 2),
                    data['status']
                ]
                writer.writerow(row)
            
            self.logger.info(f"åˆ†æ‹£å‘˜æ’åæ•°æ®å·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {detail_filepath} å’Œ {summary_filepath}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜åˆ†æ‹£å‘˜æ’åCSVæ•°æ®æ—¶å‡ºé”™: {e}")
    
    def save_to_json(self, data: Dict[str, Any], date_str: str = None):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        data_dir = self.config['collection']['data_dir']
        os.makedirs(data_dir, exist_ok=True)
        
        filename = self.config['collection']['json_filename'].format(date=date_str)
        filepath = os.path.join(data_dir, filename)
        
        # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¿½åŠ åˆ°æ•°ç»„ä¸­
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                if not isinstance(existing_data, list):
                    existing_data = [existing_data]
                existing_data.append(data)
            except:
                existing_data = [data]
        else:
            existing_data = [data]
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°JSONæ–‡ä»¶: {filepath}")
    
    def save_to_csv(self, data: Dict[str, Any], date_str: str = None):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        data_dir = self.config['collection']['data_dir']
        os.makedirs(data_dir, exist_ok=True)
        
        # ä¿å­˜åŸå§‹æ•°æ®è®°å½•
        raw_filename = "raw_data.csv"
        raw_filepath = os.path.join(data_dir, raw_filename)
        
        # ä¿å­˜ç»Ÿè®¡æ±‡æ€»æ•°æ®
        summary_filename = "summary_stats.csv"
        summary_filepath = os.path.join(data_dir, summary_filename)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå†³å®šæ˜¯å¦å†™å…¥è¡¨å¤´
        raw_file_exists = os.path.exists(raw_filepath)
        summary_file_exists = os.path.exists(summary_filepath)
        
        try:
            # ä¿å­˜åŸå§‹æ•°æ®è®°å½•
            with open(raw_filepath, 'a', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                
                # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå†™å…¥è¡¨å¤´
                if not raw_file_exists:
                    headers = [
                        'é‡‡é›†æ—¶é—´', 'ç›®æ ‡æ—¥æœŸ', 'APIçŠ¶æ€ç ', 'APIæ¶ˆæ¯', 
                        'å“åº”çŠ¶æ€', 'åŸå§‹æ•°æ®', 'å¤‡æ³¨'
                    ]
                    writer.writerow(headers)
                
                # å†™å…¥åŸå§‹æ•°æ®è¡Œ
                api_data = data.get('data', {})
                row = [
                    data['timestamp'],
                    data['target_date'],
                    api_data.get('code', ''),
                    api_data.get('msg', ''),
                    data['status'],
                    json.dumps(api_data, ensure_ascii=False),
                    data.get('error', '')
                ]
                writer.writerow(row)
            
            # å¦‚æœAPIè¿”å›æˆåŠŸä¸”æœ‰æ•°æ®ï¼Œå°è¯•è§£æç»Ÿè®¡ä¿¡æ¯
            if data['status'] == 'success' and isinstance(api_data, dict) and api_data.get('code') == 0:
                api_response_data = api_data.get('data')
                
                # æ ¹æ®æ‚¨æä¾›çš„å›¾ç‰‡ï¼Œè®¾è®¡ç»Ÿè®¡æ•°æ®è¡¨å¤´
                with open(summary_filepath, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    
                    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå†™å…¥ç»Ÿè®¡è¡¨å¤´
                    if not summary_file_exists:
                        summary_headers = [
                            'é‡‡é›†æ—¶é—´', 'ç›®æ ‡æ—¥æœŸ',
                            'æ€»ä»»åŠ¡æ•°', 'å·²å®Œæˆä»»åŠ¡æ•°', 'ç¼ºè´§ä»»åŠ¡æ•°', 'æœªå®Œæˆä»»åŠ¡æ•°',
                            'è®¡é‡ä»»åŠ¡æ•°', 'å•†å“ç§ç±»æ•°', 'ä¸è®¡é‡ä»»åŠ¡æ•°', 'å•†æˆ·æ•°',
                            # åˆ†ç±»è¿›åº¦ç»Ÿè®¡ï¼ˆåŸºäºAPIè¿”å›çš„çœŸå®åˆ†ç±»ï¼‰
                            'æ–°é²œè”¬èœ_æœªå®Œæˆ', 'æ–°é²œè”¬èœ_å·²å®Œæˆ', 'æ–°é²œè”¬èœ_ç¼ºè´§',
                            'æ–°é²œè‚‰ç±»_æœªå®Œæˆ', 'æ–°é²œè‚‰ç±»_å·²å®Œæˆ', 'æ–°é²œè‚‰ç±»_ç¼ºè´§',
                            'é²œæ´»æ°´äº§_æœªå®Œæˆ', 'é²œæ´»æ°´äº§_å·²å®Œæˆ', 'é²œæ´»æ°´äº§_ç¼ºè´§',
                            'æ—¶ä»¤æœè”¬_æœªå®Œæˆ', 'æ—¶ä»¤æœè”¬_å·²å®Œæˆ', 'æ—¶ä»¤æœè”¬_ç¼ºè´§',
                            'é²œæ´»ç¦½ç±»_æœªå®Œæˆ', 'é²œæ´»ç¦½ç±»_å·²å®Œæˆ', 'é²œæ´»ç¦½ç±»_ç¼ºè´§',
                            'ä¼‘é—²é£Ÿå“_æœªå®Œæˆ', 'ä¼‘é—²é£Ÿå“_å·²å®Œæˆ', 'ä¼‘é—²é£Ÿå“_ç¼ºè´§',
                            'é€Ÿå†»é€Ÿé£Ÿ_æœªå®Œæˆ', 'é€Ÿå†»é€Ÿé£Ÿ_å·²å®Œæˆ', 'é€Ÿå†»é€Ÿé£Ÿ_ç¼ºè´§',
                            'å—åŒ—å¹²è´§_æœªå®Œæˆ', 'å—åŒ—å¹²è´§_å·²å®Œæˆ', 'å—åŒ—å¹²è´§_ç¼ºè´§',
                            'å¨æˆ¿é…±æ–™_æœªå®Œæˆ', 'å¨æˆ¿é…±æ–™_å·²å®Œæˆ', 'å¨æˆ¿é…±æ–™_ç¼ºè´§',
                            'ä¹³å“çƒ˜ç„™_æœªå®Œæˆ', 'ä¹³å“çƒ˜ç„™_å·²å®Œæˆ', 'ä¹³å“çƒ˜ç„™_ç¼ºè´§',
                            'å¨æˆ¿ç”¨å“_æœªå®Œæˆ', 'å¨æˆ¿ç”¨å“_å·²å®Œæˆ', 'å¨æˆ¿ç”¨å“_ç¼ºè´§',
                            'ç±³é¢ç²®æ²¹_æœªå®Œæˆ', 'ç±³é¢ç²®æ²¹_å·²å®Œæˆ', 'ç±³é¢ç²®æ²¹_ç¼ºè´§',
                            'è…Šå‘³ç†Ÿé£Ÿ_æœªå®Œæˆ', 'è…Šå‘³ç†Ÿé£Ÿ_å·²å®Œæˆ', 'è…Šå‘³ç†Ÿé£Ÿ_ç¼ºè´§',
                            'å…¶ä»–_æœªå®Œæˆ', 'å…¶ä»–_å·²å®Œæˆ', 'å…¶ä»–_ç¼ºè´§',
                            # å…¶ä»–ç»Ÿè®¡å­—æ®µ
                            'æ€»é‡é‡(kg)', 'å¹³å‡é‡é‡(kg)', 'APIå“åº”æ—¶é—´(ms)', 'æ•°æ®å®Œæ•´æ€§'
                        ]
                        writer.writerow(summary_headers)
                    
                    # è§£æå¹¶å†™å…¥ç»Ÿè®¡æ•°æ®
                    if api_response_data and isinstance(api_response_data, dict):
                        # ä»APIå“åº”ä¸­æå–ç»Ÿè®¡æ•°æ®ï¼Œä¼ å…¥å®Œæ•´çš„APIæ•°æ®
                        stats = self.parse_statistics(api_data)
                        
                        summary_row = [
                            data['timestamp'],
                            data['target_date'],
                            stats.get('total_tasks', 0),
                            stats.get('completed_tasks', 0),
                            stats.get('shortage_tasks', 0),
                            stats.get('uncompleted_tasks', 0),
                            stats.get('weight_tasks', 0),
                            stats.get('product_types', 0),
                            stats.get('no_weight_tasks', 0),
                            stats.get('merchant_count', 0),
                            # åˆ†ç±»ç»Ÿè®¡æ•°æ®ï¼ˆæŒ‰ç…§æ–°çš„è¡¨å¤´é¡ºåºï¼‰
                            stats.get('æ–°é²œè”¬èœ_æœªå®Œæˆ', 0), stats.get('æ–°é²œè”¬èœ_å·²å®Œæˆ', 0), stats.get('æ–°é²œè”¬èœ_ç¼ºè´§', 0),
                            stats.get('æ–°é²œè‚‰ç±»_æœªå®Œæˆ', 0), stats.get('æ–°é²œè‚‰ç±»_å·²å®Œæˆ', 0), stats.get('æ–°é²œè‚‰ç±»_ç¼ºè´§', 0),
                            stats.get('é²œæ´»æ°´äº§_æœªå®Œæˆ', 0), stats.get('é²œæ´»æ°´äº§_å·²å®Œæˆ', 0), stats.get('é²œæ´»æ°´äº§_ç¼ºè´§', 0),
                            stats.get('æ—¶ä»¤æœè”¬_æœªå®Œæˆ', 0), stats.get('æ—¶ä»¤æœè”¬_å·²å®Œæˆ', 0), stats.get('æ—¶ä»¤æœè”¬_ç¼ºè´§', 0),
                            stats.get('é²œæ´»ç¦½ç±»_æœªå®Œæˆ', 0), stats.get('é²œæ´»ç¦½ç±»_å·²å®Œæˆ', 0), stats.get('é²œæ´»ç¦½ç±»_ç¼ºè´§', 0),
                            stats.get('ä¼‘é—²é£Ÿå“_æœªå®Œæˆ', 0), stats.get('ä¼‘é—²é£Ÿå“_å·²å®Œæˆ', 0), stats.get('ä¼‘é—²é£Ÿå“_ç¼ºè´§', 0),
                            stats.get('é€Ÿå†»é€Ÿé£Ÿ_æœªå®Œæˆ', 0), stats.get('é€Ÿå†»é€Ÿé£Ÿ_å·²å®Œæˆ', 0), stats.get('é€Ÿå†»é€Ÿé£Ÿ_ç¼ºè´§', 0),
                            stats.get('å—åŒ—å¹²è´§_æœªå®Œæˆ', 0), stats.get('å—åŒ—å¹²è´§_å·²å®Œæˆ', 0), stats.get('å—åŒ—å¹²è´§_ç¼ºè´§', 0),
                            stats.get('å¨æˆ¿é…±æ–™_æœªå®Œæˆ', 0), stats.get('å¨æˆ¿é…±æ–™_å·²å®Œæˆ', 0), stats.get('å¨æˆ¿é…±æ–™_ç¼ºè´§', 0),
                            stats.get('ä¹³å“çƒ˜ç„™_æœªå®Œæˆ', 0), stats.get('ä¹³å“çƒ˜ç„™_å·²å®Œæˆ', 0), stats.get('ä¹³å“çƒ˜ç„™_ç¼ºè´§', 0),
                            stats.get('å¨æˆ¿ç”¨å“_æœªå®Œæˆ', 0), stats.get('å¨æˆ¿ç”¨å“_å·²å®Œæˆ', 0), stats.get('å¨æˆ¿ç”¨å“_ç¼ºè´§', 0),
                            stats.get('ç±³é¢ç²®æ²¹_æœªå®Œæˆ', 0), stats.get('ç±³é¢ç²®æ²¹_å·²å®Œæˆ', 0), stats.get('ç±³é¢ç²®æ²¹_ç¼ºè´§', 0),
                            stats.get('è…Šå‘³ç†Ÿé£Ÿ_æœªå®Œæˆ', 0), stats.get('è…Šå‘³ç†Ÿé£Ÿ_å·²å®Œæˆ', 0), stats.get('è…Šå‘³ç†Ÿé£Ÿ_ç¼ºè´§', 0),
                            stats.get('å…¶ä»–_æœªå®Œæˆ', 0), stats.get('å…¶ä»–_å·²å®Œæˆ', 0), stats.get('å…¶ä»–_ç¼ºè´§', 0),
                            # å…¶ä»–ç»Ÿè®¡
                            stats.get('total_weight', 0),
                            stats.get('avg_weight', 0),
                            stats.get('response_time', 0),
                            stats.get('data_integrity', 'å®Œæ•´')
                        ]
                        writer.writerow(summary_row)
                    else:
                        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œå†™å…¥ç©ºè¡Œæˆ–é”™è¯¯ä¿¡æ¯
                        empty_row = [data['timestamp'], data['target_date']] + [''] * (len(summary_headers) - 2)
                        writer.writerow(empty_row)
            
            self.logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {raw_filepath}")
            if data['status'] == 'success':
                self.logger.info(f"ç»Ÿè®¡æ•°æ®å·²ä¿å­˜åˆ°: {summary_filepath}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")
    
    def parse_statistics(self, api_data: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æAPIå“åº”æ•°æ®ï¼Œæå–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_tasks': 0,
            'completed_tasks': 0,
            'shortage_tasks': 0,
            'uncompleted_tasks': 0,
            'weight_tasks': 0,
            'product_types': 0,
            'no_weight_tasks': 0,
            'merchant_count': 0,
            'total_weight': 0,
            'avg_weight': 0,
            'response_time': 0,
            'data_integrity': 'å®Œæ•´'
        }
        
        try:
            # æ ¹æ®çœŸå®APIå“åº”ç»“æ„è§£ææ•°æ®
            if 'data' in api_data and api_data['data']:
                data = api_data['data']
                
                # è§£æåˆ†ç±»è°ƒåº¦æ•°æ® (category_schedule)
                if 'category_schedule' in data and isinstance(data['category_schedule'], list):
                    category_data = data['category_schedule']
                    
                    for category in category_data:
                        category_id = category.get('id', '')
                        total_count = category.get('total_count', 0)
                        finished_count = category.get('finished_count', 0)
                        unfinished_count = category.get('unfinished_count', 0)
                        out_of_stock_count = category.get('out_of_stock_count', 0)
                        
                        # æ˜ å°„åˆ†ç±»IDåˆ°ä¸­æ–‡åç§°ï¼ˆä½¿ç”¨APIè¿”å›çš„nameå­—æ®µï¼‰
                        category_name = category.get('name', 'å…¶ä»–')
                         
                        # å¦‚æœæ²¡æœ‰nameå­—æ®µï¼Œä½¿ç”¨IDæ˜ å°„
                        if not category_name or category_name == 'å…¶ä»–':
                            category_name_map = {
                                'A627108': 'æ–°é²œè”¬èœ',
                                'A627109': 'æ–°é²œè‚‰ç±»', 
                                'A627111': 'é²œæ´»æ°´äº§',
                                'A627113': 'æ—¶ä»¤æœè”¬',
                                'A627110': 'é²œæ´»ç¦½ç±»',
                                'A627118': 'ä¼‘é—²é£Ÿå“',
                                'A627112': 'é€Ÿå†»é€Ÿé£Ÿ',
                                'A627115': 'å—åŒ—å¹²è´§',
                                'A627119': 'å¨æˆ¿é…±æ–™',
                                'A627114': 'ä¹³å“çƒ˜ç„™',
                                'A629184': 'å¨æˆ¿ç”¨å“',
                                'A627117': 'ç±³é¢ç²®æ²¹',
                                'A627116': 'è…Šå‘³ç†Ÿé£Ÿ'
                            }
                            category_name = category_name_map.get(category_id, 'å…¶ä»–')
                        
                        # è®¾ç½®åˆ†ç±»ç»Ÿè®¡
                        stats[f'{category_name}_æœªå®Œæˆ'] = unfinished_count
                        stats[f'{category_name}_å·²å®Œæˆ'] = finished_count
                        stats[f'{category_name}_ç¼ºè´§'] = out_of_stock_count
                        
                        # ç´¯è®¡æ€»æ•°
                        stats['total_tasks'] += total_count
                        stats['completed_tasks'] += finished_count
                        stats['uncompleted_tasks'] += unfinished_count
                        stats['shortage_tasks'] += out_of_stock_count
                
                # è§£ææ’åºæ•°æ® (sort_data)
                if 'sort_data' in data:
                    sort_data = data['sort_data']
                    stats['merchant_count'] = sort_data.get('address_count', 0)
                    stats['product_types'] = sort_data.get('sku_count', 0)
                    stats['no_weight_tasks'] = sort_data.get('unweight_count', 0)
                    stats['weight_tasks'] = sort_data.get('weight_count', 0)
                
                # è§£ææ€»è°ƒåº¦æ•°æ® (total_schedule)
                if 'total_schedule' in data:
                    total_schedule = data['total_schedule']
                    # éªŒè¯æ€»æ•°æ˜¯å¦ä¸€è‡´
                    api_total = total_schedule.get('total_count', 0)
                    api_finished = total_schedule.get('finished_count', 0)
                    api_unfinished = total_schedule.get('unfinished_count', 0)
                    api_out_of_stock = total_schedule.get('out_of_stock_count', 0)
                    
                    # ä½¿ç”¨APIæä¾›çš„æ€»æ•°ï¼ˆæ›´å‡†ç¡®ï¼‰
                    if api_total > 0:
                        stats['total_tasks'] = api_total
                        stats['completed_tasks'] = api_finished
                        stats['uncompleted_tasks'] = api_unfinished
                        stats['shortage_tasks'] = api_out_of_stock
                
                # è®¡ç®—å¹³å‡é‡é‡ï¼ˆå¦‚æœæœ‰é‡é‡æ•°æ®ï¼‰
                if stats['weight_tasks'] > 0 and stats['total_weight'] > 0:
                    stats['avg_weight'] = round(stats['total_weight'] / stats['weight_tasks'], 2)
                
                self.logger.info(f"æˆåŠŸè§£æç»Ÿè®¡æ•°æ®: æ€»ä»»åŠ¡{stats['total_tasks']}, å·²å®Œæˆ{stats['completed_tasks']}, æœªå®Œæˆ{stats['uncompleted_tasks']}, ç¼ºè´§{stats['shortage_tasks']}")
            
        except Exception as e:
            self.logger.error(f"è§£æç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
            stats['data_integrity'] = f'è§£æé”™è¯¯: {str(e)}'
        
        return stats
    
    def collect_once(self):
        """æ‰§è¡Œä¸€æ¬¡æ•°æ®é‡‡é›†"""
        self.logger.info("=" * 60)
        self.logger.info("å¼€å§‹æ‰§è¡Œæ•°æ®é‡‡é›†...")
        self.logger.info(f"é‡‡é›†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # è·å–åˆ†æ‹£è¿›åº¦æ•°æ®
        data = self.fetch_data()
        
        # ä¿å­˜åˆ†æ‹£è¿›åº¦æ•°æ®
        date_str = datetime.now().strftime('%Y%m%d')
        self.save_to_json(data, date_str)
        self.save_to_csv(data, date_str)
        
        if data['status'] == 'success':
            # è§£æå¹¶æ˜¾ç¤ºåˆ†æ‹£è¿›åº¦è¯¦ç»†ä¿¡æ¯
            stats = self.parse_statistics(data.get('data', {}))
            self.logger.info("âœ“ åˆ†æ‹£è¿›åº¦æ•°æ®é‡‡é›†å®Œæˆ")
            self.logger.info(f"  ğŸ“Š æ€»ä»»åŠ¡æ•°: {stats.get('total_tasks', 0)}")
            self.logger.info(f"  âœ… å·²å®Œæˆ: {stats.get('completed_tasks', 0)}")
            self.logger.info(f"  â³ æœªå®Œæˆ: {stats.get('uncompleted_tasks', 0)}")
            self.logger.info(f"  âŒ ç¼ºè´§: {stats.get('shortage_tasks', 0)}")
            
            # è®¡ç®—å¹¶æ˜¾ç¤ºå®Œæˆç‡
            if stats.get('total_tasks', 0) > 0:
                completion_rate = round((stats.get('completed_tasks', 0) / stats.get('total_tasks', 0)) * 100, 1)
                self.logger.info(f"  ğŸ“ˆ å®Œæˆç‡: {completion_rate}%")
            
            # æ˜¾ç¤ºå•†æˆ·å’Œå•†å“ä¿¡æ¯
            if stats.get('merchant_count', 0) > 0:
                self.logger.info(f"  ğŸª å•†æˆ·æ•°: {stats.get('merchant_count', 0)}")
            if stats.get('product_types', 0) > 0:
                self.logger.info(f"  ğŸ“¦ å•†å“ç§ç±»: {stats.get('product_types', 0)}")
            
            # æ˜¾ç¤ºè®¡é‡ä¿¡æ¯
            weight_tasks = stats.get('weight_tasks', 0)
            no_weight_tasks = stats.get('no_weight_tasks', 0)
            if weight_tasks > 0 or no_weight_tasks > 0:
                self.logger.info(f"  âš–ï¸ è®¡é‡ä»»åŠ¡: {weight_tasks} | ä¸è®¡é‡ä»»åŠ¡: {no_weight_tasks}")
        else:
            self.logger.error(f"âœ— åˆ†æ‹£è¿›åº¦æ•°æ®é‡‡é›†å¤±è´¥: {data.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        self.logger.info("")  # ç©ºè¡Œåˆ†éš”
        
        # è·å–åˆ†æ‹£å‘˜æ’åæ•°æ®
        self.logger.info("å¼€å§‹è·å–åˆ†æ‹£å‘˜æ’åæ•°æ®...")
        sorter_rank_data = self.fetch_sorter_rank_data()
        
        # ä¿å­˜åˆ†æ‹£å‘˜æ’åæ•°æ®
        self.save_sorter_rank_to_json(sorter_rank_data, date_str)
        self.save_sorter_rank_to_csv(sorter_rank_data, date_str)
        
        if sorter_rank_data['status'] == 'success':
            # æ˜¾ç¤ºåˆ†æ‹£å‘˜æ’åè¯¦ç»†ä¿¡æ¯
            api_data = sorter_rank_data.get('data', {})
            if api_data.get('code') == 0 and isinstance(api_data.get('data'), list):
                sorters = api_data['data']
                total_completed = sum(sorter.get('statistic_results', 0) for sorter in sorters)
                self.logger.info("âœ“ åˆ†æ‹£å‘˜æ’åæ•°æ®é‡‡é›†å®Œæˆ")
                self.logger.info(f"  ğŸ‘¥ åˆ†æ‹£å‘˜æ€»æ•°: {len(sorters)}")
                self.logger.info(f"  ğŸ“¦ æ€»å®Œæˆä»¶æ•°: {total_completed}")
                if sorters:
                    avg_completed = round(total_completed / len(sorters), 1) if len(sorters) > 0 else 0
                    self.logger.info(f"  ğŸ“Š å¹³å‡å®Œæˆä»¶æ•°: {avg_completed}")
                    # æ˜¾ç¤ºå‰3ååˆ†æ‹£å‘˜
                    top_sorters = sorted(sorters, key=lambda x: x.get('statistic_results', 0), reverse=True)[:3]
                    self.logger.info("  ğŸ† æ’åå‰ä¸‰:")
                    for i, sorter in enumerate(top_sorters, 1):
                        self.logger.info(f"    {i}. {sorter.get('sorter_name', 'æœªçŸ¥')} - {sorter.get('statistic_results', 0)}ä»¶")
        else:
            self.logger.error(f"âœ— åˆ†æ‹£å‘˜æ’åæ•°æ®é‡‡é›†å¤±è´¥: {sorter_rank_data.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        # é‡‡é›†å®Œæˆæ€»ç»“
        self.logger.info("")  # ç©ºè¡Œåˆ†éš”
        overall_status = 'success' if data['status'] == 'success' and sorter_rank_data['status'] == 'success' else 'partial_success'
        if overall_status == 'success':
            self.logger.info("ğŸ‰ æœ¬æ¬¡æ•°æ®é‡‡é›†å…¨éƒ¨å®Œæˆ!")
        else:
            self.logger.info("âš ï¸ æœ¬æ¬¡æ•°æ®é‡‡é›†éƒ¨åˆ†å®Œæˆ")
        
        self.logger.info("=" * 60)
        self.logger.info("")  # æœ€åçš„ç©ºè¡Œåˆ†éš”
        
        return {
            'sorting_progress': data,
            'sorter_ranking': sorter_rank_data,
            'status': overall_status,
            'timestamp': datetime.now().isoformat()
        }
    
    def start_scheduled_collection(self):
        """å¯åŠ¨å®šæ—¶é‡‡é›†"""
        interval = self.config['collection']['interval_minutes']
        self.logger.info(f"å¯åŠ¨å®šæ—¶æ•°æ®é‡‡é›†ï¼Œé—´éš”: {interval} åˆ†é’Ÿ")
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡
        self.collect_once()
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every(interval).minutes.do(self.collect_once)
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(1)
        except KeyboardInterrupt:
            self.logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
        except Exception as e:
            self.logger.error(f"å®šæ—¶ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    print("ç°åœºéƒ¨-åˆ†æ‹£è¿›åº¦æ•°æ®é‡‡é›†å™¨")
    print("=" * 50)
    
    collector = DataCollector()
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == 'test':
            print("\næ‰§è¡Œæµ‹è¯•æ¨¡å¼ - å•æ¬¡æ•°æ®é‡‡é›†...")
            result = collector.collect_once()
            print(f"é‡‡é›†ç»“æœ: {result['status']}")
            if result['status'] == 'success':
                print("âœ“ æ•°æ®é‡‡é›†æˆåŠŸ")
            else:
                print(f"âœ— æ•°æ®é‡‡é›†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return
        elif sys.argv[1] == 'schedule':
            print(f"\nå¯åŠ¨å®šæ—¶é‡‡é›† (é—´éš”: {collector.config['collection']['interval_minutes']} åˆ†é’Ÿ)")
            print("æŒ‰ Ctrl+C åœæ­¢é‡‡é›†")
            collector.start_scheduled_collection()
            return
    
    print("é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. æ‰§è¡Œä¸€æ¬¡é‡‡é›†")
    print("2. å¯åŠ¨å®šæ—¶é‡‡é›†")
    print("3. ç›´æ¥å¯åŠ¨å®šæ—¶é‡‡é›†ï¼ˆé»˜è®¤ï¼‰")
    print()
    print("æç¤ºï¼šç›´æ¥æŒ‰å›è½¦å°†å¯åŠ¨å®šæ—¶é‡‡é›†æ¨¡å¼")
    
    try:
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2 æˆ–ç›´æ¥å›è½¦): ").strip()
        
        if choice == '1':
            print("\næ‰§è¡Œå•æ¬¡æ•°æ®é‡‡é›†...")
            result = collector.collect_once()
            print(f"é‡‡é›†ç»“æœ: {result['status']}")
            if result['status'] == 'success':
                print("âœ“ æ•°æ®é‡‡é›†æˆåŠŸ")
            else:
                print(f"âœ— æ•°æ®é‡‡é›†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
        elif choice == '2' or choice == '' or choice == '3':
            print(f"\nå¯åŠ¨å®šæ—¶é‡‡é›† (é—´éš”: {collector.config['collection']['interval_minutes']} åˆ†é’Ÿ)")
            print("æŒ‰ Ctrl+C åœæ­¢é‡‡é›†")
            collector.start_scheduled_collection()
            
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤å¯åŠ¨å®šæ—¶é‡‡é›†...")
            print(f"\nå¯åŠ¨å®šæ—¶é‡‡é›† (é—´éš”: {collector.config['collection']['interval_minutes']} åˆ†é’Ÿ)")
            print("æŒ‰ Ctrl+C åœæ­¢é‡‡é›†")
            collector.start_scheduled_collection()
            
    except KeyboardInterrupt:
        print("\nç¨‹åºå·²åœæ­¢")
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")

if __name__ == "__main__":
    main()